[TOC]

## Basics and Tuning Techniques of General Neural Networks

### General
* Random Initialization of the weight matrix *W* and *b*:
    - Motivation: symmetry breaking
    - Behavior: If all the parameters are initialized by the same values, then for all the hidden layers, for any input *x* we have $$a_{1}=a_{2}=....a_{m}$$ for any hidden layer in the NN.



### Forward Pass Algorithm

### BackPropagation Algorithm

