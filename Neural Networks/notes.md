[TOC]

## Basics and Tuning Techniques of General Neural Networks

### General
* Random Initialization of the weight matrix *W* and *b*:
    - Motivation: symmetry breaking
    - Behavior: If all the parameters are initialized by the same values, then for all the hidden layers, for any input *x* we have $$a_{1}=a_{2}=....a_{m}$$ for any hidden layer in the NN.
* Regularization:
    - Motivation: To avoid the overfitting.
    - Behavior: Add a weight decay item with a decaying parameter $$\lambda$$ to the cost function.
* Mini-Batch:
* Stochastic Gradient Descent:


### Forward Pass Algorithm


### BackPropagation Algorithm


### Common Interview Questions


### Questions
* Why matrix operation is helpful for computational efficiency?
